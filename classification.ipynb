{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBbAp_Oiw0hi"
      },
      "source": [
        "# Instruction\n",
        "Plase use <b>Python 3</b> in Jupyter Notebook.\n",
        "\n",
        "This lab focuses on classifcation and prediction. We will practice classification methods on a real world wetland mapping dataset. Each data sample contains several numeric features and a binary class label (0 for dry land, 1 for wetland).\n",
        "\n",
        "<b>Requirement</b>\n",
        "- <font color=red>Plese upload your Jupyter Notebook with required Data files and Python script files all in the SAME zipped FOLDER</font>\n",
        "\n",
        "- Please MAKE SURE your codes run smoothly without bugs in Jupyter Notebook with Python 3.\n",
        "    \n",
        "- <font color=red>Codes with bugs or errors that cannot run through in Jupyter notebook will be graded as ZERO for that part.</font>\n",
        "\n",
        "## Statement of Contribution\n",
        "\n",
        "We both tried the homework on our own. Sumanthra was responsible for developing the programming solutions and the extra credit problem. Karthik completed the written assignment. Both partners were involved in reviewing the code and written components."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vT4XdKtnw0hk"
      },
      "outputs": [],
      "source": [
        "# Load libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import BaggingClassifier # Bagging Classifier\n",
        "from sklearn.ensemble import RandomForestClassifier # Random Forest Classifier\n",
        "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
        "from sklearn.linear_model import LogisticRegression # Import Logistic Regression Classifier\n",
        "from sklearn.svm import SVC # Import SVM classifier\n",
        "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
        "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQYoNvYGw0hl"
      },
      "source": [
        "## Data loading\n",
        "Please use the following codes to load the data. The training and test data are both saved in common separated values (CSV) format. The last column is class label.\n",
        "\n",
        "PLEASE RUN THE CODES BELOW."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gq0KT2o4w0hl",
        "outputId": "0e595fce-4647-4c05-e92e-59c462be9a33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   near infra red   red  green  blue  class\n",
            "0              123  132    115   133      0\n",
            "1              152  150    119   187      1\n",
            "2              169  166    143   192      1\n",
            "3               55   49     43    97      0\n",
            "4              141  135    117   181      1\n",
            "   near infra red   red  green  blue  class\n",
            "0              137  140    129   150      0\n",
            "1              169  162    140   193      1\n",
            "2              124  110     89   162      1\n",
            "3              105  104     99   153      1\n",
            "4              105  102     88   173      1\n"
          ]
        }
      ],
      "source": [
        "#note: class 0 for dry land, class 1 for wetland\n",
        "col_names = ['near infra red ', 'red', 'green', 'blue', 'class']\n",
        "features =  ['near infra red ', 'red', 'green', 'blue']\n",
        "\n",
        "# load dataset\n",
        "train_dat = pd.read_csv(\"train.csv\", header=None, names=col_names)\n",
        "print(train_dat.head())\n",
        "\n",
        "test_dat = pd.read_csv(\"test.csv\", header=None, names=col_names)\n",
        "print(test_dat.head())\n",
        "\n",
        "# COMPLETE CODES BELOW, compute training features X_train, training labels Y_train\n",
        "X_train = train_dat.drop('class', axis='columns')\n",
        "Y_train = train_dat['class']\n",
        "\n",
        "# COMPLETE CODES BELOW, compute test features X_test, test labels Y_test\n",
        "X_test = test_dat.drop('class', axis='columns')\n",
        "Y_test = test_dat['class']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "id": "lg_FfSwL5EFR",
        "outputId": "1d007b0e-3c36-470a-d567-be228ea73a2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "class\n",
            "0    560\n",
            "1    440\n",
            "Name: count, dtype: int64\n",
            "['Dryland', 'Wetland'] [560 440]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "counts = train_dat['class'].value_counts()\n",
        "print(counts)\n",
        "\n",
        "\n",
        "# Get the labels and counts from the value_counts() result\n",
        "categories = counts.index\n",
        "values = counts.values\n",
        "\n",
        "# Map numerical labels to meaningful names\n",
        "labels = ['Dryland' if landtype == 0 else 'Wetland' for landtype in categories]\n",
        "print(labels,values)\n",
        "\n",
        "# Create the bar plot\n",
        "plt.bar(labels, values, color=['blue', 'green'])\n",
        "\n",
        "# Add labels and a title for clarity\n",
        "plt.xlabel('Land Type')\n",
        "plt.ylabel('Number of Values')\n",
        "plt.title('Distribution of Dryland vs. Wetland')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjPqkFE4w0hl"
      },
      "source": [
        "## Part 1: Evaluate the performance of decision tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UxpKC5Yw0hl"
      },
      "source": [
        "Run the codes below to train a decision tree, and make predictions on test samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "CW5krjw9w0hm"
      },
      "outputs": [],
      "source": [
        "# Create Decision Tree classifer object\n",
        "dt = DecisionTreeClassifier()\n",
        "\n",
        "# Train Decision Tree Classifer\n",
        "dt = dt.fit(X_train,Y_train)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "Y_pred = dt.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpkSD6zJw0hm"
      },
      "source": [
        "Run the codes below to compute the \"Overall Accuracy\", as well as Precision, Recall, and F-score for the Wetland class (class 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6FIY5FZw0hm",
        "outputId": "bea17b95-1542-4e57-9c19-e56f5ec85ed8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.783\n",
            "0.7390350877192983\n",
            "0.774712643678161\n",
            "0.7564534231200898\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# accuracy = metrics.accuracy_score(Y_train, Y_pred)\n",
        "# precision_wet = metrics.precision_score(Y_train, Y_pred)\n",
        "# recall_wet = metrics.recall_score(Y_train, Y_pred)\n",
        "# F_wet = metrics.f1_score(Y_train, Y_pred)\n",
        "\n",
        "# print('Metrics of Model on Training Data')\n",
        "# print(accuracy)\n",
        "# print(precision_wet)\n",
        "# print(recall_wet)\n",
        "# print(F_wet)\n",
        "\n",
        "\n",
        "# Fill in codes to calculate the values below; You can add any codes, but don't change the variable names\n",
        "\n",
        "accuracy = metrics.accuracy_score(Y_test, Y_pred)\n",
        "\n",
        "precision_wet = metrics.precision_score(Y_test, Y_pred)\n",
        "\n",
        "recall_wet = metrics.recall_score(Y_test, Y_pred)\n",
        "\n",
        "F_wet = metrics.f1_score(Y_test, Y_pred)\n",
        "\n",
        "print(accuracy)\n",
        "print(precision_wet)\n",
        "print(recall_wet)\n",
        "print(F_wet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDb6OPy8w0hm"
      },
      "source": [
        "<font color=red>ANSWER THE QUESTION BELOW.</font> How do you think about the performance shown by different metrics above. Is accuracy a good metric to reflect classification performance? Why? You can discuss your answer as a string. And print it out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "r8WsKi0V1Xzy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkXsWzVcw0hm",
        "outputId": "3a6ea853-fe5a-4ea7-bddd-b6e00b4e16fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "MY Answer: Accuracy is giving a decent representation of the performane since the wetland and dryland classes are not misbalanced by A heavy margin.\n",
            "But accuracy alone is not the best metric because accuracy could look high even if the model performs poorly on a minority class, \n",
            "Precision, recall, and F1-score give a clearer picture of performance. \n",
            "In environmental classification tasks, recall (sensitivity) for wetlands is particularly important, since missing wetlands can have serious ecological consequences.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#MY ANSWER IS xxxxx\n",
        "ans = '''\n",
        "MY Answer: Accuracy is giving a decent representation of the performane since the wetland and dryland classes are not misbalanced by A heavy margin.\n",
        "But accuracy alone is not the best metric because accuracy could look high even if the model performs poorly on a minority class,\n",
        "Precision, recall, and F1-score give a clearer picture of performance.\n",
        "In environmental classification tasks, recall (sensitivity) for wetlands is particularly important, since missing wetlands can have serious ecological consequences.\n",
        "\n",
        "'''\n",
        "print(ans)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX8y-tnGw0hn"
      },
      "source": [
        "## Part 2: Overfitting issues\n",
        "<font color=red>PLEASE COMPLETE TEH CODES BELOW.</font> Please re-train the decision tree model with a smaller number of training samples. <font color=red>DO NOT change the tree model parameters (e.g., minimum leaf node size) from Part 1</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3kOfdbtw0hn",
        "outputId": "15ab584e-0d9b-42a7-f615-fa8e558dd441"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   near infra red   red  green  blue  class\n",
            "0              143  150    145   156      0\n",
            "1              151  143    120   183      1\n",
            "2              100   98     86   156      0\n",
            "3              140  137    119   182      0\n",
            "4              147  147    138   160      0\n",
            "   near infra red   red  green  blue  class\n",
            "0              137  140    129   150      0\n",
            "1              169  162    140   193      1\n",
            "2              124  110     89   162      1\n",
            "3              105  104     99   153      1\n",
            "4              105  102     88   173      1\n",
            "Metrics of Model on Training Data\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "1.0\n",
            "Metrics of Model on Test Data\n",
            "0.697\n",
            "0.6586538461538461\n",
            "0.6298850574712643\n",
            "0.6439482961222092\n"
          ]
        }
      ],
      "source": [
        "# Re-run the training and testing based on a smaller training set (smalltrain.csv).\n",
        "#note: class 0 for dry land, class 1 for wetland\n",
        "col_names = ['near infra red ', 'red', 'green', 'blue', 'class']\n",
        "features =  ['near infra red ', 'red', 'green', 'blue']\n",
        "\n",
        "# load dataset\n",
        "train_dat = pd.read_csv(\"smalltrain.csv\", header=None, names=col_names)\n",
        "print(train_dat.head())\n",
        "\n",
        "test_dat = pd.read_csv(\"test.csv\", header=None, names=col_names)\n",
        "print(test_dat.head())\n",
        "\n",
        "# COMPLETE CODES BELOW, compute training features X_train, training labels Y_train\n",
        "X_train = train_dat.drop('class', axis='columns')\n",
        "Y_train = train_dat['class']\n",
        "\n",
        "# COMPLETE CODES BELOW, compute test features X_test, test labels Y_test\n",
        "X_test = test_dat.drop('class',axis='columns')\n",
        "Y_test = test_dat['class']\n",
        "\n",
        "# Create Decision Tree classifer object\n",
        "dt = DecisionTreeClassifier()\n",
        "\n",
        "# Train Decision Tree Classifer\n",
        "dt = dt.fit(X_train,Y_train)\n",
        "\n",
        "# Re-evaluate the trained model on test data, print out accuracy, precision, recall, F-score\n",
        "Y_pred = dt.predict(X_train)\n",
        "accuracy = metrics.accuracy_score(Y_train, Y_pred)\n",
        "precision_wet = metrics.precision_score(Y_train, Y_pred)\n",
        "recall_wet = metrics.recall_score(Y_train, Y_pred)\n",
        "F_wet = metrics.f1_score(Y_train, Y_pred)\n",
        "\n",
        "print('Metrics of Model on Training Data')\n",
        "print(accuracy)\n",
        "print(precision_wet)\n",
        "print(recall_wet)\n",
        "print(F_wet)\n",
        "\n",
        "Y_pred = dt.predict(X_test)\n",
        "accuracy = metrics.accuracy_score(Y_test,Y_pred)\n",
        "precision_wet = metrics.precision_score(Y_test,Y_pred)\n",
        "recall_wet = metrics.recall_score(Y_test,Y_pred)\n",
        "F_wet = metrics.f1_score(Y_test,Y_pred)\n",
        "\n",
        "print('Metrics of Model on Test Data')\n",
        "print(accuracy)\n",
        "print(precision_wet)\n",
        "print(recall_wet)\n",
        "print(F_wet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbF3YhlTw0hn"
      },
      "source": [
        "<font color=red>ANSWER THE QUESTION BELOW.</font> What do you observe when the number of training samples decrese? Is this overfitting or underfitting?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYLTJPX2w0hn",
        "outputId": "310ca9f0-e3dc-4c54-8bf7-b23ac9f7a273"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "MY Answer: The model is overfitting because, the training accuracy and other metrics are 1 but when trying to generalise,\n",
            "the test accuracy,precision,recall,f1 scores are much lower. This indicates that the decision tree has overfitted\n",
            "over the training data\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#MY Answer: xxxxxxxxxxx\n",
        "ans = '''\n",
        "MY Answer: The model is overfitting because, the training accuracy and other metrics are 1 but when trying to generalise,\n",
        "the test accuracy,precision,recall,f1 scores are much lower. This indicates that the decision tree has overfitted\n",
        "over the training data\n",
        "'''\n",
        "\n",
        "print(ans)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bQtQ3naw0hn"
      },
      "source": [
        "We now do another experiment to mitigate the effect of overfitting by decreasing the model complexity. We keep using the small training data (smalltrain.csv) that leads to overfitting. We decrease model complexity by having a larger minimum leaf node size (\"min_sample_split=30\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "irXHbVvIw0hn",
        "outputId": "32ec94d6-f514-4699-ed80-d86038b7569c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   near infra red   red  green  blue  class\n",
            "0              143  150    145   156      0\n",
            "1              151  143    120   183      1\n",
            "2              100   98     86   156      0\n",
            "3              140  137    119   182      0\n",
            "4              147  147    138   160      0\n",
            "   near infra red   red  green  blue  class\n",
            "0              137  140    129   150      0\n",
            "1              169  162    140   193      1\n",
            "2              124  110     89   162      1\n",
            "3              105  104     99   153      1\n",
            "4              105  102     88   173      1\n",
            "Metrics of Model on Test Data\n",
            "0.757\n",
            "0.6889763779527559\n",
            "0.8045977011494253\n",
            "0.7423117709437964\n"
          ]
        }
      ],
      "source": [
        "col_names = ['near infra red ', 'red', 'green', 'blue', 'class']\n",
        "features =  ['near infra red ', 'red', 'green', 'blue']\n",
        "\n",
        "# load dataset\n",
        "train_dat = pd.read_csv(\"smalltrain.csv\", header=None, names=col_names)\n",
        "print(train_dat.head())\n",
        "\n",
        "test_dat = pd.read_csv(\"test.csv\", header=None, names=col_names)\n",
        "print(test_dat.head())\n",
        "\n",
        "# COMPLETE CODES BELOW, compute training features X_train, training labels Y_train\n",
        "X_train = train_dat.drop('class', axis='columns')\n",
        "Y_train = train_dat['class']\n",
        "\n",
        "# COMPLETE CODES BELOW, compute test features X_test, test labels Y_test\n",
        "X_test = test_dat.drop('class', axis='columns')\n",
        "Y_test = test_dat['class']\n",
        "\n",
        "# Create Decision Tree classifer object\n",
        "dt_simple = DecisionTreeClassifier(min_samples_split=30) ### Make model much simplier by requiring 30 samples to split\n",
        "\n",
        "# Train Decision Tree Classifer\n",
        "dt_simple = dt_simple.fit(X_train, Y_train)\n",
        "\n",
        "#Predict the response for test dataset\n",
        "Y_pred = dt_simple.predict(X_test)\n",
        "\n",
        "\n",
        "accuracy = metrics.accuracy_score(Y_test, Y_pred)\n",
        "precision_wet = metrics.precision_score(Y_test, Y_pred)\n",
        "recall_wet = metrics.recall_score(Y_test, Y_pred)\n",
        "F_wet = metrics.f1_score(Y_test, Y_pred)\n",
        "\n",
        "\n",
        "print('Metrics of Model on Test Data')\n",
        "print(accuracy)\n",
        "print(precision_wet)\n",
        "print(recall_wet)\n",
        "print(F_wet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXoMLYogw0hn"
      },
      "source": [
        "<font color=red>ANSWER THE QUESTION BELOW.</font> Compare the results above with the results from the beginning of Part 2 (smalltrain.csv with the original decision tree without decreasing model complexity), what did you observe?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jtj2oGaWw0hn",
        "outputId": "278d9342-ffa0-493d-c70b-dab31a113c79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "MY Answer:\n",
            "By decreasing model complexity with min_samples_split=30, we can see that the test performance improved compared to the overfitted model. \n",
            "The regularized model shows more balanced performance with 0.757 test accuracy, which is better than the overfitted version.\n",
            "Also, the regularized model achieved much higher recall (0.807 vs 0.667), meaning it's better at correctly identifying wetlands. \n",
            "The improvement in F1-Score(0.743 vs 0.668) also shows better overall balance between precision and recall.\n",
            "This demonstrates that reducing model complexity can help mitigate overfitting and improve generalization across all evaluation metrics.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#MY ANSWER IS xxxxx\n",
        "ans = '''\n",
        "MY Answer:\n",
        "By decreasing model complexity with min_samples_split=30, we can see that the test performance improved compared to the overfitted model.\n",
        "The regularized model shows more balanced performance with 0.757 test accuracy, which is better than the overfitted version.\n",
        "Also, the regularized model achieved much higher recall (0.807 vs 0.667), meaning it's better at correctly identifying wetlands.\n",
        "The improvement in F1-Score(0.743 vs 0.668) also shows better overall balance between precision and recall.\n",
        "This demonstrates that reducing model complexity can help mitigate overfitting and improve generalization across all evaluation metrics.\n",
        "'''\n",
        "print(ans)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIbxOrbnw0hn"
      },
      "source": [
        "## Part 3: Compare different model on the same test data\n",
        "In this part, you will train other types of models and evaluate on the test data. You will compare their classification performance.\n",
        "\n",
        "<font color=red>PLEASE COMPLETE TEH CODES BELOW.</font> PLEASE USE THE SAME TRAINING AND TEST DATA as in Part 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xBl3L-Jw0hn",
        "outputId": "60590605-82a9-43e3-d4e4-6ff9a0d43734"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   near infra red   red  green  blue  class\n",
            "0              123  132    115   133      0\n",
            "1              152  150    119   187      1\n",
            "2              169  166    143   192      1\n",
            "3               55   49     43    97      0\n",
            "4              141  135    117   181      1\n",
            "   near infra red   red  green  blue  class\n",
            "0              137  140    129   150      0\n",
            "1              169  162    140   193      1\n",
            "2              124  110     89   162      1\n",
            "3              105  104     99   153      1\n",
            "4              105  102     88   173      1\n",
            "0.757\n",
            "0.7307692307692307\n",
            "0.6988505747126437\n",
            "0.7144535840188014\n"
          ]
        }
      ],
      "source": [
        "col_names = ['near infra red ', 'red', 'green', 'blue', 'class']\n",
        "features =  ['near infra red ', 'red', 'green', 'blue']\n",
        "\n",
        "# load dataset\n",
        "train_dat = pd.read_csv(\"train.csv\", header=None, names=col_names)\n",
        "print(train_dat.head())\n",
        "\n",
        "test_dat = pd.read_csv(\"test.csv\", header=None, names=col_names)\n",
        "print(test_dat.head())\n",
        "\n",
        "# COMPLETE CODES BELOW, compute training features X_train, training labels Y_train\n",
        "X_train = train_dat.drop('class', axis='columns')\n",
        "Y_train = train_dat['class']\n",
        "\n",
        "# COMPLETE CODES BELOW, compute test features X_test, test labels Y_test\n",
        "X_test = test_dat.drop('class', axis='columns')\n",
        "Y_test = test_dat['class']\n",
        "\n",
        "# train a logistic regression model; e.g., use the LogisticRegression model in Scikit-Learn.\n",
        "# using parameter \"solver='liblinear'\"\n",
        "lr = LogisticRegression(solver='liblinear')\n",
        "lr = lr.fit(X_train, Y_train)\n",
        "Y_pred = lr.predict(X_test)\n",
        "\n",
        "# evalute the logistic regression model on test data\n",
        "accuracy = metrics.accuracy_score(Y_test, Y_pred)\n",
        "precision_wet = metrics.precision_score(Y_test, Y_pred)\n",
        "recall_wet = metrics.recall_score(Y_test, Y_pred)\n",
        "F_wet = metrics.f1_score(Y_test, Y_pred)\n",
        "\n",
        "print(accuracy)\n",
        "print(precision_wet)\n",
        "print(recall_wet)\n",
        "print(F_wet)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z02ikdfPw0ho",
        "outputId": "5b00065b-dc0e-41e4-9e3a-b5b5b3a33aff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.81\n",
            "0.7406679764243614\n",
            "0.8666666666666667\n",
            "0.798728813559322\n"
          ]
        }
      ],
      "source": [
        "# train a Support Vector Machine (SVM) model, e.g., the SVC model in Scikit-Learn, choose parameters approprioately\n",
        "\n",
        "# Please find the SVC function in Scikit-Learn, use parameters \"gamma='scale', C=100\"\n",
        "clf = SVC(gamma='scale', C=100)\n",
        "clf.fit(X_train, Y_train)\n",
        "\n",
        "# evalute the Support Vector Machine (SVM) model on test data\n",
        "Y_pred = clf.predict(X_test)\n",
        "accuracy = metrics.accuracy_score(Y_test, Y_pred)\n",
        "precision_wet = metrics.precision_score(Y_test, Y_pred)\n",
        "recall_wet = metrics.recall_score(Y_test, Y_pred)\n",
        "F_wet = metrics.f1_score(Y_test, Y_pred)\n",
        "\n",
        "print(accuracy)\n",
        "print(precision_wet)\n",
        "print(recall_wet)\n",
        "print(F_wet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juB9LaBqw0ho"
      },
      "source": [
        "<font color=red>ANSWER THE QUESTION BELOW.</font> How do you compare the results from different models above with decision tree?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75HezHXiw0ho",
        "outputId": "63be62e8-93f3-4b83-e3b9-550930873069"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "My Answer: Comparing the different models on the test data:\n",
            "1. Decision Tree (Part 1 i.e before overfitting): 0.782 accuracy, 0.737 precision, 0.775 recall, 0.756 F1\n",
            "2. Logistic Regression: 0.757 accuracy, 0.731 precision, 0.699 recall, 0.714 F1  \n",
            "3. Support Vector Machine: 0.810 accuracy, 0.741 precision, 0.867 recall, 0.799 F1\n",
            "\n",
            "The SVM model performed best overall with the highest accuracy (0.810) and F1-score (0.799).\n",
            "SVM also had the best recall (0.867) for wetland detection, which is important for environmental applications.\n",
            "The decision tree performed moderately well, while logistic regression had the lowest performance among the three.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ans = '''\n",
        "My Answer: Comparing the different models on the test data:\n",
        "1. Decision Tree (Part 1 i.e before overfitting): 0.782 accuracy, 0.737 precision, 0.775 recall, 0.756 F1\n",
        "2. Logistic Regression: 0.757 accuracy, 0.731 precision, 0.699 recall, 0.714 F1\n",
        "3. Support Vector Machine: 0.810 accuracy, 0.741 precision, 0.867 recall, 0.799 F1\n",
        "\n",
        "The SVM model performed best overall with the highest accuracy (0.810) and F1-score (0.799).\n",
        "SVM also had the best recall (0.867) for wetland detection, which is important for environmental applications.\n",
        "The decision tree performed moderately well, while logistic regression had the lowest performance among the three.\n",
        "'''\n",
        "\n",
        "print(ans)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZBFij--w0ho"
      },
      "source": [
        "## Part 4: Ensemble learning\n",
        "In this part, you will run several ensemble learning of decision trees, including bagging and random forest. For random forest, you can directly call it as a separate model from library.\n",
        "\n",
        "<font color=red>PLEASE COMPLETE CODES BELOW</font>. PLEASE USE THE ORIGINAL TRAINING AND TEST DATA in Part 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTuYs3ngw0ho",
        "outputId": "45bb7d79-2a79-4ae8-de51-396a5c359744"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   near infra red   red  green  blue  class\n",
            "0              123  132    115   133      0\n",
            "1              152  150    119   187      1\n",
            "2              169  166    143   192      1\n",
            "3               55   49     43    97      0\n",
            "4              141  135    117   181      1\n",
            "   near infra red   red  green  blue  class\n",
            "0              137  140    129   150      0\n",
            "1              169  162    140   193      1\n",
            "2              124  110     89   162      1\n",
            "3              105  104     99   153      1\n",
            "4              105  102     88   173      1\n",
            "0.811\n",
            "0.7900943396226415\n",
            "0.7701149425287356\n",
            "0.779976717112922\n"
          ]
        }
      ],
      "source": [
        "#note: class 0 for dry land, class 1 for wetland\n",
        "col_names = ['near infra red ', 'red', 'green', 'blue', 'class']\n",
        "features =  ['near infra red ', 'red', 'green', 'blue']\n",
        "\n",
        "# load dataset\n",
        "train_dat = pd.read_csv(\"train.csv\", header=None, names=col_names)\n",
        "print(train_dat.head())\n",
        "\n",
        "test_dat = pd.read_csv(\"test.csv\", header=None, names=col_names)\n",
        "print(test_dat.head())\n",
        "\n",
        "# COMPLETE CODES BELOW, compute training features X_train, training labels Y_train\n",
        "X_train = train_dat.drop('class', axis='columns')\n",
        "Y_train = train_dat['class']\n",
        "\n",
        "# COMPLETE CODES BELOW, compute test features X_test, test labels Y_test\n",
        "X_test = test_dat.drop('class', axis='columns')\n",
        "Y_test = test_dat['class']\n",
        "\n",
        "# please train bagging of decision tree, using BaggingClassifier with default parameters\n",
        "clf = BaggingClassifier()\n",
        "clf = clf.fit(X_train, Y_train)\n",
        "\n",
        "# please evaluate it on the test data\n",
        "Y_pred = clf.predict(X_test)\n",
        "accuracy = metrics.accuracy_score(Y_test, Y_pred)\n",
        "precision_wet = metrics.precision_score(Y_test, Y_pred)\n",
        "recall_wet = metrics.recall_score(Y_test, Y_pred)\n",
        "F_wet = metrics.f1_score(Y_test, Y_pred)\n",
        "\n",
        "print(accuracy)\n",
        "print(precision_wet)\n",
        "print(recall_wet)\n",
        "print(F_wet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G48rPEIcw0ho",
        "outputId": "f631ee7a-085e-49fc-9345-541958f6f502"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.809\n",
            "0.7772727272727272\n",
            "0.7862068965517242\n",
            "0.7817142857142857\n"
          ]
        }
      ],
      "source": [
        "# please train a random forest, using RandomForestClassifier function with paramters \"n_estimators=50\"\n",
        "clf = RandomForestClassifier(n_estimators=50)\n",
        "clf = clf.fit(X_train, Y_train)\n",
        "\n",
        "# please evaluate it on the test data\n",
        "Y_pred = clf.predict(X_test)\n",
        "accuracy = metrics.accuracy_score(Y_test, Y_pred)\n",
        "precision_wet = metrics.precision_score(Y_test, Y_pred)\n",
        "recall_wet = metrics.recall_score(Y_test, Y_pred)\n",
        "F_wet = metrics.f1_score(Y_test, Y_pred)\n",
        "\n",
        "print(accuracy)\n",
        "print(precision_wet)\n",
        "print(recall_wet)\n",
        "print(F_wet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "fAF_Pw1woIb9"
      },
      "outputs": [],
      "source": [
        "# print(Y_pred.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPiZk1CCw0ho"
      },
      "source": [
        "PLEASE ANSWER THE QUESTION BELOW. How do you compare different ensemble methods? Which one has the best performance?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPyIN2zyw0ho",
        "outputId": "422fae4b-2a54-41fa-d648-fce67ad496a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "MY ANSWER:\n",
            "Bagging achieves higher accuracy (0.811 vs. 0.809) and precision (0.790 vs. 0.777), meaning it is slightly better at avoiding false positives. \n",
            "However, Random Forest achieves higher recall (0.786 vs. 0.770) and a marginally better F1 score (0.782 vs. 0.780), indicating it is \n",
            "more effective at correctly identifying positives while maintaining a balanced trade-off between precision and recall. \n",
            "Since the F1 score is generally the best measure of overall performance, Random Forest can be considered the stronger method in this comparison, \n",
            "though the difference is minimal.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "ans = '''\n",
        "MY ANSWER:\n",
        "Bagging achieves higher accuracy (0.811 vs. 0.809) and precision (0.790 vs. 0.777), meaning it is slightly better at avoiding false positives.\n",
        "However, Random Forest achieves higher recall (0.786 vs. 0.770) and a marginally better F1 score (0.782 vs. 0.780), indicating it is\n",
        "more effective at correctly identifying positives while maintaining a balanced trade-off between precision and recall.\n",
        "Since the F1 score is generally the best measure of overall performance, Random Forest can be considered the stronger method in this comparison,\n",
        "though the difference is minimal.\n",
        "'''\n",
        "\n",
        "print(ans)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6U52GDyXw0hp"
      },
      "source": [
        "## Part 5 (EXTRA CREDIT): Implement your own classifier (logistic regression)\n",
        "\n",
        "You should implement your logistic regression model: a training function and a test function. Save your source codes in 'myLR.py'. Put the python script in the same dirctory of this Jupyter Notebook. Load the scripts so that you can call your own training and prediction functions. Then evaluate the results on test data. Compare your results from the results from built-in logistic regression library in Part 4. Please use the same training data and test data as Part 1. Please make sure you print out the accuracy, confusion matrix, precision, recall, F-score.\n",
        "\n",
        "<b>Requirement</b>\n",
        "\n",
        "- Your codes should run through without bugs. Codes with bugs in Jupyter notebook running will NOT be graded.\n",
        "    \n",
        "- You CANNOT copy a same python codes from online for this question. It will be treated as cheating.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8pafAAFWD2P"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class MyLR:\n",
        "    \"\"\"\n",
        "    Enhanced Logistic Regression with regularization and adaptive learning\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, learning_rate=0.5, max_iterations=3000, tolerance=1e-8, reg_lambda=0.01):\n",
        "        \"\"\"\n",
        "        Initialize with improved hyperparameters\n",
        "\n",
        "        Parameters:\n",
        "        learning_rate: Higher initial learning rate for faster convergence\n",
        "        max_iterations: More iterations for better optimization\n",
        "        tolerance: Convergence threshold\n",
        "        reg_lambda: L2 regularization parameter to prevent overfitting\n",
        "        \"\"\"\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_iterations = max_iterations\n",
        "        self.tolerance = tolerance\n",
        "        self.reg_lambda = reg_lambda\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.cost_history = []\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "\n",
        "        z = np.clip(z, -500, 500)\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def add_polynomial_features(self, X):\n",
        "        \n",
        "        # Add polynomial features (squares and interactions) to capture non-linear patterns\n",
        "        \n",
        "        X_poly = X.copy()\n",
        "        n_features = X.shape[1]\n",
        "\n",
        "        # Add squared terms\n",
        "        for i in range(n_features):\n",
        "            X_poly = np.column_stack([X_poly, X[:, i] ** 2])\n",
        "\n",
        "        # Add interaction terms between features\n",
        "        for i in range(n_features):\n",
        "            for j in range(i+1, n_features):\n",
        "                X_poly = np.column_stack([X_poly, X[:, i] * X[:, j]])\n",
        "\n",
        "        return X_poly\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Train with feature engineering and adaptive learning rate\n",
        "        \"\"\"\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            X = X.values\n",
        "        if isinstance(y, pd.Series):\n",
        "            y = y.values\n",
        "\n",
        "        # Add polynomial features for better non-linear modeling\n",
        "        X = self.add_polynomial_features(X)\n",
        "\n",
        "        # Normalize features\n",
        "        self.mean_ = np.mean(X, axis=0)\n",
        "        self.std_ = np.std(X, axis=0)\n",
        "        self.std_ = np.where(self.std_ == 0, 1, self.std_)\n",
        "        X_normalized = (X - self.mean_) / self.std_\n",
        "\n",
        "        n_samples, n_features = X_normalized.shape\n",
        "\n",
        "        # Better initialization using Xavier/He initialization\n",
        "        np.random.seed(42)\n",
        "        self.weights = np.random.normal(0, np.sqrt(2.0/n_features), n_features)\n",
        "        self.bias = 0.0\n",
        "\n",
        "        # Adaptive learning rate\n",
        "        initial_lr = self.learning_rate\n",
        "\n",
        "        # Gradient descent with momentum\n",
        "        velocity_w = np.zeros(n_features)\n",
        "        velocity_b = 0.0\n",
        "        momentum = 0.9\n",
        "\n",
        "        prev_cost = float('inf')\n",
        "        patience = 0\n",
        "        best_weights = self.weights.copy()\n",
        "        best_bias = self.bias\n",
        "        best_cost = float('inf')\n",
        "\n",
        "        for i in range(self.max_iterations):\n",
        "            # Forward pass\n",
        "            linear_pred = np.dot(X_normalized, self.weights) + self.bias\n",
        "            predictions = self.sigmoid(linear_pred)\n",
        "\n",
        "            # Compute cost with L2 regularization\n",
        "            cost = self.cross_entropy_loss(y, predictions)\n",
        "            reg_cost = cost + (self.reg_lambda / (2 * n_samples)) * np.sum(self.weights ** 2)\n",
        "            self.cost_history.append(reg_cost)\n",
        "\n",
        "            # Tracking the best model\n",
        "            if reg_cost < best_cost:\n",
        "                best_cost = reg_cost\n",
        "                best_weights = self.weights.copy()\n",
        "                best_bias = self.bias\n",
        "                patience = 0\n",
        "            else:\n",
        "                patience += 1\n",
        "\n",
        "            # Compute gradients with L2 regularization\n",
        "            dw = (1/n_samples) * np.dot(X_normalized.T, (predictions - y))\n",
        "            dw += (self.reg_lambda / n_samples) * self.weights  # L2 regularization\n",
        "            db = (1/n_samples) * np.sum(predictions - y)\n",
        "\n",
        "            # Momentum update\n",
        "            velocity_w = momentum * velocity_w - self.learning_rate * dw\n",
        "            velocity_b = momentum * velocity_b - self.learning_rate * db\n",
        "\n",
        "            self.weights += velocity_w\n",
        "            self.bias += velocity_b\n",
        "\n",
        "            # Adaptive learning rate decay\n",
        "            if i > 0 and i % 500 == 0:\n",
        "                self.learning_rate *= 0.95\n",
        "\n",
        "            # Check convergence\n",
        "            if abs(prev_cost - reg_cost) < self.tolerance:\n",
        "                print(f\"Converged after {i+1} iterations\")\n",
        "                break\n",
        "\n",
        "            # Early stopping if no improvement\n",
        "            if patience > 200:\n",
        "                print(f\"Early stopping at iteration {i+1}\")\n",
        "                self.weights = best_weights\n",
        "                self.bias = best_bias\n",
        "                break\n",
        "\n",
        "            prev_cost = reg_cost\n",
        "\n",
        "            if (i + 1) % 500 == 0:\n",
        "                print(f\"Iteration {i+1}, Cost: {reg_cost:.6f}, LR: {self.learning_rate:.4f}\")\n",
        "\n",
        "        # Use best model found\n",
        "        self.weights = best_weights\n",
        "        self.bias = best_bias\n",
        "\n",
        "        return self\n",
        "\n",
        "    def cross_entropy_loss(self, y_true, y_pred):\n",
        "        epsilon = 1e-15\n",
        "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
        "        cost = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "        return cost\n",
        "\n",
        "    def predict(self, X):\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            X = X.values\n",
        "\n",
        "        # Apply same feature engineering\n",
        "        X = self.add_polynomial_features(X)\n",
        "        X_normalized = (X - self.mean_) / self.std_\n",
        "        linear_pred = np.dot(X_normalized, self.weights) + self.bias\n",
        "        probabilities = self.sigmoid(linear_pred)\n",
        "        # Can tune threshold if needed (default 0.5)\n",
        "        predictions = (probabilities >= 0.5).astype(int)\n",
        "\n",
        "        print(f\"Probability range: [{probabilities.min():.3f}, {probabilities.max():.3f}]\")\n",
        "        print(f\"Predictions distribution: {np.bincount(predictions)}\")\n",
        "\n",
        "        return predictions\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nSAIod-yoVjq",
        "outputId": "ac98247c-f8d3-4f32-8f19-3afe2dbf79e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   near infra red   red  green  blue  class\n",
            "0              123  132    115   133      0\n",
            "1              152  150    119   187      1\n",
            "2              169  166    143   192      1\n",
            "3               55   49     43    97      0\n",
            "4              141  135    117   181      1\n",
            "   near infra red   red  green  blue  class\n",
            "0              137  140    129   150      0\n",
            "1              169  162    140   193      1\n",
            "2              124  110     89   162      1\n",
            "3              105  104     99   153      1\n",
            "4              105  102     88   173      1\n",
            "Iteration 500, Cost: 0.448086, LR: 0.5000\n",
            "Iteration 1000, Cost: 0.429929, LR: 0.4750\n",
            "Iteration 1500, Cost: 0.422707, LR: 0.4512\n",
            "Iteration 2000, Cost: 0.418896, LR: 0.4287\n",
            "Iteration 2500, Cost: 0.416492, LR: 0.4073\n",
            "Iteration 3000, Cost: 0.414796, LR: 0.3869\n",
            "Probability range: [0.000, 1.000]\n",
            "Predictions distribution: [515 485]\n",
            "[[440 125]\n",
            " [ 75 360]]\n",
            "0.8\n",
            "0.7422680412371134\n",
            "0.8275862068965517\n",
            "0.782608695652174\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "col_names = ['near infra red ', 'red', 'green', 'blue', 'class']\n",
        "features =  ['near infra red ', 'red', 'green', 'blue']\n",
        "\n",
        "# load dataset\n",
        "train_dat = pd.read_csv(\"train.csv\", header=None, names=col_names)\n",
        "print(train_dat.head())\n",
        "\n",
        "test_dat = pd.read_csv(\"test.csv\", header=None, names=col_names)\n",
        "print(test_dat.head())\n",
        "\n",
        "# COMPLETE CODES BELOW, compute training features X_train, training labels Y_train\n",
        "X_train = train_dat.drop('class',axis = 'columns')\n",
        "Y_train = train_dat['class']\n",
        "\n",
        "# COMPLETE CODES BELOW, compute test features X_test, test labels Y_test\n",
        "X_test = test_dat.drop('class',axis='columns')\n",
        "Y_test = test_dat['class']\n",
        "\n",
        "\n",
        "# Call your own function to train a logistic regression model, as what we did in Part 1 with decision tree\n",
        "myLR = MyLR() #Initaliising with default parameters\n",
        "myLR = myLR.fit(X_train, Y_train)\n",
        "\n",
        "# Call your own function to make prediction on test data, and evaluate those metrics, as what we did in Part 1 with deicsion tree\n",
        "Y_pred = myLR.predict(X_test)\n",
        "# print(Y_pred.head())\n",
        "# print(Y_pred.shape)\n",
        "\n",
        "# evalute the logistic regression model on test data\n",
        "confusion_matrix = metrics.confusion_matrix(Y_test, Y_pred)\n",
        "accuracy = metrics.accuracy_score(Y_test, Y_pred)\n",
        "precision_wet = metrics.precision_score(Y_test, Y_pred)\n",
        "recall_wet = metrics.recall_score(Y_test, Y_pred)\n",
        "F_wet = metrics.f1_score(Y_test, Y_pred)\n",
        "\n",
        "print(confusion_matrix)\n",
        "print(accuracy)\n",
        "print(precision_wet)\n",
        "print(recall_wet)\n",
        "print(F_wet)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WUhAkbft1DG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
